{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/miniconda3/lib/python3.7/site-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from urllib.request import Request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "wb = load_workbook(filename='LinkNYC_Coverage_Tracker.xlsx')\n",
    "ws = wb['2019'] # ws is now an IterableWorksheet\n",
    "\n",
    "# iterate thru all cells and if hyperlink found attempt modification of cell\n",
    "for row in ws.rows:\n",
    "    for cell in row:\n",
    "        try:\n",
    "            if len(cell.hyperlink.target)  > 0:\n",
    "                cell.value = \"\".join([cell.value,\"|\",cell.hyperlink.target])\n",
    "                # Join cell.value and hyperlink target into string (optionally just assign the hyperlink.target to the cell.value\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# save workbook to temp .xlsx (I could not manage to read from buffer...) .\n",
    "wb.save(\"temp.xlsx\")\n",
    "\n",
    "# read with pandas \n",
    "data = pd.read_excel(\"temp.xlsx\")\n",
    "\n",
    "# take DataSeries and rsplit by \"|\" and expand to 2 columns\n",
    "hyper = (data.Headline.str.rsplit(\"|\", expand=True))\n",
    "\n",
    "#set labels\n",
    "hyper.columns=[\"Label\",\"Hyperlink\"]\n",
    "\n",
    "# join them back to dataframe on index\n",
    "data = data.join(hyper, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdata = data[['Date','Area of Business','Headline','Sentiment (Positive, Negative, Neutral)','Label', 'Hyperlink']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "hyperdata.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb1 = load_workbook(filename='LinkNYC_Coverage_Tracker.xlsx')\n",
    "ws1 = wb1['2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/miniconda3/lib/python3.7/site-packages/pandas/util/_decorators.py:188: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "ws_dict = []\n",
    "sheetName = ['2018','2017','2016','2015','Greatest Hits']\n",
    "for sheetn in sheetName:\n",
    "    ws_dict.append(pd.read_excel('LinkNYC_Coverage_Tracker.xlsx', sheetname=sheetn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink = []\n",
    "for i in range(len(ws_dict)):\n",
    "    hyperlink.append(ws_dict[i].URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "corpus_2018 = []\n",
    "for i in range(len(hyperlink[0])):\n",
    "    try:\n",
    "        req = Request(hyperlink[0][i], headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract() \n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        corpus_2018.append(text)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [x.encode('utf-8') for x in corpus_2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_2018.txt\", \"w\") as output:\n",
    "    output.write(str(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "corpus_2017 = []\n",
    "for i in range(len(hyperlink[1])):\n",
    "    try:\n",
    "        req = Request(hyperlink[1][i], headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract() \n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        corpus_2017.append(text)\n",
    "    except:\n",
    "        pass\n",
    "corpus2017 = [x.encode('utf-8') for x in corpus_2017]\n",
    "with open(\"text_2017.txt\", \"w\") as output:\n",
    "    output.write(str(corpus2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_2015 = []\n",
    "for i in range(len(hyperlink[3])):\n",
    "    try:\n",
    "        req = Request(hyperlink[3][i], headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract() \n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        corpus_2015.append(text)\n",
    "    except:\n",
    "        pass\n",
    "corpus2015 = [x.encode('utf-8') for x in corpus_2015]\n",
    "with open(\"text_2015.txt\", \"w\") as output:\n",
    "    output.write(str(corpus2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Greatest_Hits = []\n",
    "for i in range(len(hyperlink[4])):\n",
    "    try:\n",
    "        req = Request(hyperlink[4][i], headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract() \n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        Greatest_Hits.append(text)\n",
    "    except:\n",
    "        pass\n",
    "Greatest_Hits = [x.encode('utf-8') for x in Greatest_Hits]\n",
    "with open(\"Greatest_Hits.txt\", \"w\") as output:\n",
    "    output.write(str(Greatest_Hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocess(text):\n",
    "    return re.sub(r\"[^a-zA-Z0-9]+\", ' ', re.sub(r'\\d+', '', text.lower()).translate(str.maketrans('', '', string.punctuation)).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_2015.txt', 'r') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Greatest_Hits.txt', 'r') as f:\n",
    "    text_grehit = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_2017.txt', 'r') as f:\n",
    "    text_2017 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_2018.txt', 'r') as f:\n",
    "    text_2018 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_want = ['de','xexx','xcxa','u','le','la','en','xexxbnback','also','said','york']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_model(corpus):\n",
    "    corpus_topic = []\n",
    "    for sent in corpus:\n",
    "        corpus_topic.append(prepocess(corpus[0]))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(str(corpus_topic))\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    result = [i for i in result if not i in not_want]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    preprocess_text = []\n",
    "    for word in result:\n",
    "        preprocess_text.append(lemmatizer.lemmatize(word))\n",
    "    dictionary = gensim.corpora.Dictionary([d.split() for d in preprocess_text])\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in[d.split() for d in preprocess_text]]\n",
    "    lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 6, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_model(text):\n",
    "    corpus_topic = []\n",
    "    for sent in text:\n",
    "        corpus_topic.append(prepocess(text[0]))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(str(corpus_topic))\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    result = [i for i in result if not i in not_want]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    preprocess_text = []\n",
    "    for word in result:\n",
    "        preprocess_text.append(lemmatizer.lemmatize(word))\n",
    "    dictionary = gensim.corpora.Dictionary([d.split() for d in preprocess_text])\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in[d.split() for d in preprocess_text]]\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=6, id2word=dictionary)\n",
    "    pyLDAvis.enable_notebook()\n",
    "    pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/miniconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.save_html(pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary), 'lda_2018.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.058*\"wifi\" + 0.049*\"new\" + 0.022*\"public\" + 0.020*\"day\" + 0.020*\"booth\"')\n",
      "(1, '0.011*\"business\" + 0.010*\"good\" + 0.008*\"would\" + 0.007*\"find\" + 0.007*\"home\"')\n",
      "(2, '0.015*\"get\" + 0.011*\"news\" + 0.010*\"company\" + 0.009*\"digital\" + 0.009*\"gigabit\"')\n",
      "(3, '0.011*\"finally\" + 0.010*\"million\" + 0.009*\"hotspot\" + 0.009*\"say\" + 0.009*\"right\"')\n",
      "(4, '0.026*\"free\" + 0.019*\"first\" + 0.015*\"access\" + 0.012*\"service\" + 0.011*\"next\"')\n",
      "(5, '0.034*\"city\" + 0.024*\"phone\" + 0.023*\"hub\" + 0.020*\"kiosk\" + 0.017*\"woman\"')\n"
     ]
    }
   ],
   "source": [
    "top_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.095*\"new\" + 0.024*\"wifi\" + 0.020*\"free\" + 0.012*\"call\" + 0.011*\"technology\"')\n",
      "(1, '0.011*\"street\" + 0.011*\"use\" + 0.010*\"network\" + 0.009*\"user\" + 0.008*\"one\"')\n",
      "(2, '0.058*\"city\" + 0.013*\"first\" + 0.013*\"digital\" + 0.011*\"public\" + 0.010*\"news\"')\n",
      "(3, '0.030*\"linknyc\" + 0.018*\"phone\" + 0.017*\"display\" + 0.016*\"people\" + 0.013*\"year\"')\n",
      "(4, '0.066*\"kiosk\" + 0.024*\"time\" + 0.018*\"make\" + 0.018*\"company\" + 0.014*\"service\"')\n",
      "(5, '0.016*\"information\" + 0.015*\"main\" + 0.014*\"link\" + 0.010*\"business\" + 0.010*\"photo\"')\n"
     ]
    }
   ],
   "source": [
    "top_model(text_grehit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.025*\"pm\" + 0.015*\"time\" + 0.015*\"nyc\" + 0.014*\"june\" + 0.013*\"news\"')\n",
      "(1, '0.016*\"free\" + 0.015*\"ny\" + 0.014*\"make\" + 0.011*\"use\" + 0.011*\"toronto\"')\n",
      "(2, '0.015*\"data\" + 0.014*\"park\" + 0.013*\"get\" + 0.013*\"work\" + 0.012*\"business\"')\n",
      "(3, '0.081*\"city\" + 0.079*\"new\" + 0.018*\"linknyc\" + 0.015*\"street\" + 0.010*\"day\"')\n",
      "(4, '0.023*\"kiosk\" + 0.017*\"year\" + 0.016*\"smart\" + 0.015*\"company\" + 0.015*\"technology\"')\n",
      "(5, '0.020*\"one\" + 0.018*\"service\" + 0.017*\"public\" + 0.017*\"u\" + 0.015*\"project\"')\n"
     ]
    }
   ],
   "source": [
    "top_model(text_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.021*\"data\" + 0.018*\"free\" + 0.018*\"linknyc\" + 0.014*\"medium\" + 0.013*\"puerto\"')\n",
      "(1, '0.079*\"city\" + 0.013*\"business\" + 0.011*\"privacy\" + 0.011*\"would\" + 0.011*\"need\"')\n",
      "(2, '0.022*\"service\" + 0.019*\"smart\" + 0.015*\"phone\" + 0.010*\"way\" + 0.010*\"june\"')\n",
      "(3, '0.044*\"kiosk\" + 0.026*\"wifi\" + 0.026*\"public\" + 0.022*\"people\" + 0.017*\"one\"')\n",
      "(4, '0.075*\"new\" + 0.022*\"digital\" + 0.020*\"technology\" + 0.019*\"company\" + 0.018*\"u\"')\n",
      "(5, '0.015*\"news\" + 0.014*\"information\" + 0.010*\"policy\" + 0.009*\"community\" + 0.009*\"work\"')\n"
     ]
    }
   ],
   "source": [
    "top_model(text_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/miniconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "viz_model(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
